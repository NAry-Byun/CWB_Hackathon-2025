import asyncio
import logging
import time
import re
from datetime import datetime
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Cosmos DB ë° Azure OpenAI ì„œë¹„ìŠ¤ ì„í¬íŠ¸
from services.cosmos_service import CosmosVectorService
from services.azure_openai_service import AzureOpenAIService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()


class WebScraperService:
    """ì›¹ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³ , ì„ë² ë”© ìƒì„± ë° Cosmos DB ì €ì¥ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤"""
    def __init__(self, cosmos_service=None, openai_service=None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0'
        })
        self.cosmos_service = cosmos_service
        self.openai_service = openai_service
        self.chunk_size = 1000
        self.chunk_overlap = 200
        self.ai_keywords = [
            'artificial intelligence', 'ai', 'machine learning',
            'deep learning', 'neural network', 'nlp', 'openai'
        ]
        logger.info("ğŸ•·ï¸ WebScraperService ì´ˆê¸°í™”ë¨")

    async def scrape_and_save_url(self, url: str) -> dict:
        """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘ í›„ Cosmos DBì— ì²­í¬ ì •ë³´ì™€ ì„ë² ë”©ìœ¼ë¡œ ì €ì¥"""
        try:
            logger.info(f"ğŸ” ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            scrape_res = await self.scrape_url(url)
            if not scrape_res.get('success'):
                return scrape_res

            if not self.cosmos_service or not self.openai_service:
                return {'success': False, 'error': 'Cosmos DB ë˜ëŠ” Azure OpenAI ì„œë¹„ìŠ¤ ë¯¸ì—°ê²°'}

            save_res = await self._save_to_cosmos(scrape_res)
            scrape_res.update(save_res)
            return scrape_res

        except Exception as e:
            logger.error(f"âŒ scrape_and_save_url ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}

    async def scrape_multiple_and_save(self, urls: list) -> dict:
        """ì—¬ëŸ¬ URLì„ ìŠ¤í¬ë˜í•‘ â†’ ì²­í¬ ë¶„í•  â†’ Cosmos DB ì €ì¥"""
        results = {
            'total_urls': len(urls),
            'successful_scrapes': 0,
            'successful_saves': 0,
            'failed_urls': [],
            'processed_documents': []
        }
        for i, url in enumerate(urls):
            try:
                logger.info(f"ğŸ“„ ({i+1}/{len(urls)}) ì²˜ë¦¬: {url}")
                res = await self.scrape_and_save_url(url)

                if res.get('success'):
                    results['successful_scrapes'] += 1
                    if res.get('cosmos_saved'):
                        results['successful_saves'] += 1
                        results['processed_documents'].append({
                            'url': url,
                            'title': res.get('title'),
                            'chunks_saved': res.get('chunks_saved', 0)
                        })
                    else:
                        results['failed_urls'].append({'url': url, 'error': res.get('error')})
                else:
                    results['failed_urls'].append({'url': url, 'error': res.get('error')})

                # ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(2)
            except Exception as e:
                logger.error(f"âŒ URL ì²˜ë¦¬ ì‹¤íŒ¨: {url} | ì˜¤ë¥˜: {e}")
                results['failed_urls'].append({'url': url, 'error': str(e)})

        logger.info(f"ğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì €ì¥ ì„±ê³µ {results['successful_saves']}/{results['total_urls']}")
        return results

    async def _save_to_cosmos(self, scrape_data: dict) -> dict:
        """ìŠ¤í¬ë©ëœ ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ  ì„ë² ë”© ìƒì„± í›„ Cosmos DB ì €ì¥"""
        try:
            await self.cosmos_service.initialize_database()
            content = scrape_data.get('content', '')
            title = scrape_data.get('title', 'ì œëª© ì—†ìŒ')
            url = scrape_data.get('url')

            if not content:
                return {'cosmos_saved': False, 'error': 'ì €ì¥í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤'}

            chunks = self._split_content_into_chunks(content)
            saved_ids = []
            for idx, chunk in enumerate(chunks):
                # ì œëª©ì„ í¬í•¨í•˜ì—¬ ì²­í¬ í…ìŠ¤íŠ¸ êµ¬ì„±
                chunk_with_header = f"{title}\n{chunk}"
                emb = await self.openai_service.generate_embeddings(chunk_with_header)
                if not emb:
                    logger.error(f"âŒ ì²­í¬ {idx+1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    continue

                metadata = {
                    'source_url': url,
                    'source_title': title,
                    'scraped_at': scrape_data.get('scraped_at'),
                    'is_ai_related': scrape_data.get('is_ai_related', False),
                    'content_type': 'web_scraped',
                    'total_chunks': len(chunks)
                }
                doc_id = await self.cosmos_service.store_document_chunk(
                    file_name=f"web_{self._url_to_filename(url)}",
                    chunk_text=chunk_with_header,
                    embedding=emb,
                    chunk_index=idx,
                    metadata=metadata
                )
                saved_ids.append(doc_id)
                logger.info(f"âœ… ì²­í¬ {idx+1}/{len(chunks)} ì €ì¥ ì™„ë£Œ")

            return {'cosmos_saved': True, 'chunks_saved': len(saved_ids), 'total_chunks': len(chunks), 'document_ids': saved_ids}
        except Exception as e:
            logger.error(f"âŒ Cosmos ì €ì¥ ì‹¤íŒ¨: {e}")
            return {'cosmos_saved': False, 'error': str(e)}

    def _split_content_into_chunks(self, content: str) -> list:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ë¡œ ë¶„í• """
        sentences = re.split(r'[.!?]\s+', content)
        chunks, current = [], ''
        for sentence in sentences:
            candidate = (current + ' ' + sentence).strip() if current else sentence
            if len(candidate) <= self.chunk_size:
                current = candidate
            else:
                if current:
                    chunks.append(current)
                current = sentence
        if current:
            chunks.append(current)

        final_chunks = []
        for ch in chunks:
            if len(ch) < 100 and final_chunks:
                final_chunks[-1] += ' ' + ch
            else:
                final_chunks.append(ch)
        return final_chunks

    def _url_to_filename(self, url: str) -> str:
        parsed = urlparse(url)
        fname = parsed.netloc + parsed.path
        fname = re.sub(r'[^\w\-_.]', '_', fname)
        fname = re.sub(r'_{2,}', '_', fname)
        return fname[:100]

    async def scrape_url(self, url: str) -> dict:
        """ì‹¤ì œ í˜ì´ì§€ ìš”ì²­ í›„ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            title = self._extract_title(soup)
            content = self._extract_content(soup)
            is_ai_related = self._is_ai_related(title + ' ' + content)
            return {'success': True, 'url': url, 'title': title, 'content': content, 'is_ai_related': is_ai_related, 'scraped_at': datetime.now().isoformat()}
        except Exception as e:
            logger.error(f"âŒ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url} | ì˜¤ë¥˜: {e}")
            return {'success': False, 'url': url, 'error': str(e)}

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """h1, title, og:title ìˆœì„œë¡œ ì œëª© ì¶”ì¶œ"""
        try:
            h1 = soup.find('h1')
            if h1:
                return h1.get_text(strip=True)
            if soup.title:
                return soup.title.get_text(strip=True)
            og = soup.find('meta', property='og:title')
            if og:
                return og.get('content', '')
            return 'ì œëª© ì—†ìŒ'
        except Exception as e:
            logger.error(f"ì œëª© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return 'ì œëª© ì—†ìŒ'

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ì˜ì—­ì„ ì¶”ì¶œí•˜ê³  ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°"""
        try:
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                tag.decompose()
            selectors = ['main', 'article', '.content', '.post-content', '.entry-content', '#content']
            text = ''
            for sel in selectors:
                elem = soup.select_one(sel)
                if elem:
                    text = elem.get_text(separator=' ', strip=True)
                    break
            if not text and soup.body:
                text = soup.body.get_text(separator=' ', strip=True)
            text = re.sub(r'\s+', ' ', text).strip()
            if len(text) < 100:
                text = re.sub(r'\s+', ' ', soup.get_text(separator=' ', strip=True))
            return text[:10000]
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ''

    def _is_ai_related(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ ë‚´ AI í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ íŒë‹¨"""
        low = text.lower()
        return any(k in low for k in self.ai_keywords)

    async def health_check(self) -> dict:
        """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë° ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            r = self.session.get('https://httpbin.org/status/200', timeout=10)
            return {'status': 'healthy' if r.status_code == 200 else 'unhealthy', 'network_ok': r.status_code == 200, 'cosmos_service': self.cosmos_service is not None, 'openai_service': self.openai_service is not None}
        except Exception as e:
            logger.error(f"âŒ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
            return {'status': 'unhealthy', 'error': str(e)}


class VectorDocumentCreator:
    """ìƒ˜í”Œ ì½”ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„° ë¬¸ì„œë¡œ ìƒì„±í•˜ì—¬ Cosmos DBì— ì €ì¥í•˜ëŠ” ì„œë¹„ìŠ¤"""
    def __init__(self):
        self.cosmos_service = CosmosVectorService()
        self.openai_service = AzureOpenAIService()

    async def create_sample_vector_documents(self) -> int:
        """ìƒ˜í”Œ ë²¡í„° ë¬¸ì„œë¥¼ Cosmos DBì— ìƒì„± ë° ì €ì¥"""
        await self.cosmos_service.initialize_database()
        sample_courses = [
            {"title": "Introduction to Machine Learning", "description": "Learn the fundamentals of machine learning including supervised and unsupervised learning, neural networks, and practical applications in AI.", "category": "ai-courses", "difficulty": "beginner", "modules": 8, "duration_hours": 40},
            {"title": "Deep Learning with Neural Networks", "description": "Advanced course covering deep neural networks, convolutional networks, RNNs, and transformer architectures for AI applications.", "category": "ai-courses", "difficulty": "advanced", "modules": 12, "duration_hours": 60},
            {"title": "Natural Language Processing Fundamentals", "description": "Explore NLP techniques including tokenization, sentiment analysis, language models, and text generation using modern AI.", "category": "ai-courses", "difficulty": "intermediate", "modules": 10, "duration_hours": 50},
            {"title": "Computer Vision with AI", "description": "Learn image recognition, object detection, and computer vision applications using deep learning and AI technologies.", "category": "ai-courses", "difficulty": "intermediate", "modules": 9, "duration_hours": 45},
            {"title": "AI Ethics and Responsible AI Development", "description": "Understanding ethical considerations in AI development, bias mitigation, and responsible deployment of AI systems.", "category": "ai-courses", "difficulty": "beginner", "modules": 6, "duration_hours": 25}
        ]
        created_count = 0
        for i, course in enumerate(sample_courses):
            try:
                embedding_text = f"{course['title']}. {course['description']} Category: {course['category']}. Difficulty: {course['difficulty']}. Duration: {course['duration_hours']} hours."
                emb = await self.openai_service.generate_embeddings(embedding_text)
                if emb:
                    doc = {
                        "id": f"course_{i+1}_{course['title'].lower().replace(' ', '_')}",
                        "file_name": f"course_{i+1}",
                        "chunk_text": embedding_text,
                        "chunk_index": 0,
                        "embedding": emb,
                        "metadata": {
                            "source_type": "course_catalog", "title": course['title'], "description": course['description'], "category": course['category'], "difficulty": course['difficulty'], "modules": course['modules'], "duration_hours": course['duration_hours'], "created_at": datetime.now().isoformat()
                        }
                    }
                    await self.cosmos_service.container.create_item(body=doc)
                    created_count += 1
                    logger.info(f"âœ… ë²¡í„° ë¬¸ì„œ ìƒì„±: {course['title']}")
            except Exception as e:
                logger.error(f"âŒ ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {course['title']} | ì˜¤ë¥˜: {e}")
        logger.info(f"ğŸ‰ {created_count}ê°œì˜ ë²¡í„° ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
        return created_count

    async def test_vector_search(self) -> list:
        """ìƒ˜í”Œ ì¿¼ë¦¬ë¡œ Cosmos DB ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        try:
            test_query = "machine"
            emb = await self.openai_service.generate_embeddings(test_query)
            if emb:
                results = await self.cosmos_service.search_similar_chunks(query_embedding=emb, limit=3, similarity_threshold=0.1)
                logger.info(f"ğŸ” '{test_query}' ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê°œ:")
                for idx, res in enumerate(results, 1):
                    title = res.get('metadata', {}).get('title', 'Unknown')
                    sim = res.get('similarity', 0)
                    logger.info(f"  {idx}. {title} (ìœ ì‚¬ë„: {sim:.3f})")
                return results
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

async def main():
    # 1) ì›¹ ìŠ¤í¬ë˜í•‘ ë° Cosmos DB ì €ì¥
    cosmos = CosmosVectorService()
    openai = AzureOpenAIService()
    scraper = WebScraperService(cosmos_service=cosmos, openai_service=openai)

    urls = [
        'https://www.artificial-intelligence.blog/terminology/artificial-intelligence',
    ]
    scrape_results = await scraper.scrape_multiple_and_save(urls)
    logger.info(f"ì›¹ ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {scrape_results}")

    # 2) ìƒ˜í”Œ ì½”ìŠ¤ ë²¡í„° ë¬¸ì„œ ìƒì„±
    creator = VectorDocumentCreator()
    logger.info("ğŸ”„ ìƒ˜í”Œ ë²¡í„° ë¬¸ì„œ ìƒì„± ì‹œì‘...")
    created_count = await creator.create_sample_vector_documents()
    logger.info(f"ìƒ˜í”Œ ë²¡í„° ë¬¸ì„œ {created_count}ê°œ ìƒì„± ì™„ë£Œ")

    # 3) ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    logger.info("ğŸ” ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    await creator.test_vector_search()

    # 4) Cosmos DB í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
    await cosmos.close()

if __name__ == '__main__':
    asyncio.run(main())