"""
Enhanced Web Scraper Routes - Uses professional scraper with fallback
routes/web_scraper_routes.py
"""

import asyncio
import logging
from datetime import datetime
from flask import Blueprint, request, jsonify
from functools import wraps

logger = logging.getLogger(__name__)

# Blueprint ìƒì„±
web_scraper_bp = Blueprint('web_scraper', __name__)

# Global scraper instance
_scraper = None
_scraper_type = None

def get_scraper():
    """ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼ ìš°ì„ , ì‹¤íŒ¨ì‹œ ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼)"""
    global _scraper, _scraper_type
    
    if _scraper is None:
        try:
            # ê²½ë¡œ ì¶”ê°€
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            
            # ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼ ì‹œë„
            from services.web_scraper_service import get_scraper
            _scraper = get_scraper(use_professional=True)
            _scraper_type = type(_scraper).__name__
            logger.info(f"âœ… {_scraper_type} ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ë¬¸ ìŠ¤í¬ë˜í¼ ì‹¤íŒ¨, ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼ ì‚¬ìš©: {e}")
            try:
                # ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼ fallback
                from services.web_scraper_service import SimpleWebScraper
                _scraper = SimpleWebScraper()
                _scraper_type = "SimpleWebScraper"
            except Exception as e2:
                logger.warning(f"âš ï¸ ì„œë¹„ìŠ¤ ìŠ¤í¬ë˜í¼ë„ ì‹¤íŒ¨, ë‚´ì¥ ìŠ¤í¬ë˜í¼ ì‚¬ìš©: {e2}")
                # ì™„ì „í•œ fallback - ë‚´ì¥ ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼
                _scraper = create_fallback_scraper()
                _scraper_type = "FallbackScraper"
    
    return _scraper

def create_fallback_scraper():
    """ì™„ì „í•œ fallback ìŠ¤í¬ë˜í¼ (ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)"""
    import requests
    from bs4 import BeautifulSoup
    import re
    from datetime import datetime
    
    class FallbackScraper:
        def __init__(self):
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            self.ai_keywords = [
                'artificial intelligence', 'machine learning', 'deep learning',
                'neural network', 'ai', 'ml', 'algorithm'
            ]

        def scrape_url(self, url: str) -> dict:
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = self.extract_title(soup)
                content = self.extract_content(soup)
                
                # ë¶„ì„
                word_count = len(content.split())
                ai_related = self.is_ai_related(content, title)
                quality_score = self.calculate_quality(content, title, word_count)
                
                return {
                    'success': True,
                    'url': url,
                    'title': title,
                    'content': content[:1000] + "..." if len(content) > 1000 else content,
                    'word_count': word_count,
                    'ai_related': ai_related,
                    'quality_score': quality_score,
                    'scraped_at': datetime.now().isoformat()
                }
            except Exception as e:
                return {
                    'success': False,
                    'url': url,
                    'error': str(e)
                }

        def extract_title(self, soup):
            h1 = soup.find('h1')
            if h1 and h1.get_text(strip=True):
                return h1.get_text(strip=True)
            if soup.title:
                return soup.title.get_text(strip=True)
            return "Untitled"

        def extract_content(self, soup):
            for element in soup(['script', 'style', 'nav', 'header', 'footer']):
                element.decompose()
            
            article = soup.find('article')
            if article:
                content = article.get_text(separator=' ', strip=True)
            elif soup.body:
                content = soup.body.get_text(separator=' ', strip=True)
            else:
                content = soup.get_text(separator=' ', strip=True)
            
            content = re.sub(r'\s+', ' ', content)
            return content.strip()

        def is_ai_related(self, content, title):
            text = f"{title} {content}".lower()
            return any(keyword in text for keyword in self.ai_keywords)

        def calculate_quality(self, content, title, word_count):
            score = 0
            if word_count >= 200:
                score += 30
            if word_count >= 500:
                score += 20
            if title and len(title) > 10:
                score += 25
            if self.is_ai_related(content, title):
                score += 25
            return min(score, 100)
    
    return FallbackScraper()

def async_route(f):
    """ë¹„ë™ê¸° ë¼ìš°íŠ¸ ë°ì½”ë ˆì´í„°"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(f(*args, **kwargs))
        finally:
            loop.close()
    return wrapper

# === API ì—”ë“œí¬ì¸íŠ¸ ===

@web_scraper_bp.route('/health', methods=['GET'])
@async_route
async def scraper_health():
    """ì›¹ ìŠ¤í¬ë˜í¼ ìƒíƒœ í™•ì¸"""
    try:
        scraper = get_scraper()
        
        health_info = {
            'success': True,
            'status': 'healthy',
            'scraper_type': _scraper_type,
            'message': f'{_scraper_type}ì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤',
            'timestamp': datetime.now().isoformat()
        }
        
        # ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼ì¸ ê²½ìš° ìƒì„¸ ìƒíƒœ í™•ì¸
        if hasattr(scraper, 'health_check'):
            detailed_health = await scraper.health_check()
            health_info['detailed_status'] = detailed_health
        
        return jsonify(health_info)
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í¬ë˜í¼ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return jsonify({
            'success': False,
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@web_scraper_bp.route('/scrape', methods=['POST'])
@async_route
async def scrape_url():
    """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘"""
    try:
        data = request.get_json()
        if not data or 'url' not in data:
            return jsonify({
                'success': False,
                'error': 'URLì´ í•„ìš”í•©ë‹ˆë‹¤',
                'required_format': {'url': 'https://example.com'}
            }), 400
        
        url = data['url'].strip()
        if not url.startswith(('http://', 'https://')):
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš” (http:// ë˜ëŠ” https://ë¡œ ì‹œì‘)'
            }), 400
        
        # ì˜µì…˜ íŒŒë¼ë¯¸í„°
        store_to_db = data.get('store_to_db', True)  # ê¸°ë³¸ê°’: DBì— ì €ì¥
        use_professional = data.get('use_professional', True)  # ê¸°ë³¸ê°’: ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘
        
        scraper = get_scraper()
        
        logger.info(f"ğŸ” ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url} (ìŠ¤í¬ë˜í¼: {_scraper_type})")
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        if hasattr(scraper, 'scrape_and_store_professionally') and use_professional and store_to_db:
            # ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ + DB ì €ì¥
            result = await scraper.scrape_and_store_professionally(url)
            
            if result['success']:
                return jsonify({
                    'success': True,
                    'message': 'ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤',
                    'scraper_type': 'EnvironmentFixedScraper',
                    'data': result,
                    'features': {
                        'professional_extraction': True,
                        'quality_analysis': True,
                        'ai_relevance_scoring': True,
                        'semantic_chunking': True,
                        'rich_metadata': True,
                        'cosmos_db_storage': True
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('error', 'ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨'),
                    'details': result
                }), 400
                
        elif hasattr(scraper, 'scrape_url'):
            # ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘
            result = scraper.scrape_url(url)
            
            if result['success']:
                return jsonify({
                    'success': True,
                    'message': 'ìŠ¤í¬ë˜í•‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤',
                    'scraper_type': 'SimpleWebScraper',
                    'data': result,
                    'features': {
                        'professional_extraction': False,
                        'basic_analysis': True,
                        'cosmos_db_storage': False
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result.get('error', 'ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨')
                }), 400
        else:
            return jsonify({
                'success': False,
                'error': 'ìŠ¤í¬ë˜í¼ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
            }), 500
    
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ API ì˜¤ë¥˜: {e}")
        return jsonify({
            'success': False,
            'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@web_scraper_bp.route('/scrape/simple', methods=['POST'])
@async_route
async def scrape_url_simple():
    """ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ (DB ì €ì¥ ì—†ìŒ)"""
    try:
        data = request.get_json()
        if not data or 'url' not in data:
            return jsonify({
                'success': False,
                'error': 'URLì´ í•„ìš”í•©ë‹ˆë‹¤'
            }), 400
        
        url = data['url'].strip()
        
        # ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼ ì‚¬ìš©
        try:
            # ê²½ë¡œ ì¶”ê°€
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            
            from services.web_scraper_service import SimpleWebScraper
            simple_scraper = SimpleWebScraper()
        except Exception:
            # ì™„ì „í•œ fallback
            simple_scraper = create_fallback_scraper()
            
        result = simple_scraper.scrape_url(url)
            
        if result['success']:
                return jsonify({
                    'success': True,
                    'message': 'ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤',
                    'data': result
                })
        else:
                return jsonify({
                    'success': False,
                    'error': result.get('error', 'ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨')
                }), 400
                
    except Exception as e:
            return jsonify({
                'success': False,
                'error': f'ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼ ì˜¤ë¥˜: {str(e)}'
            }), 500
    
    except Exception as e:
        logger.error(f"âŒ ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ API ì˜¤ë¥˜: {e}")
        return jsonify({
            'success': False,
            'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'
        }), 500

@web_scraper_bp.route('/test', methods=['GET'])
@async_route
async def test_scraper():
    """ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    try:
        test_urls = [
            "https://httpbin.org/html",
            "https://www.wikipedia.org/",
            "https://www.bing.com",
            "https://www.artificial-intelligence.blog/terminology/artificial-intelligence"
        ]
        
        scraper = get_scraper()
        results = []
        
        for test_url in test_urls:
            try:
                if hasattr(scraper, 'scrape_url'):
                    result = scraper.scrape_url(test_url)
                    results.append({
                        'url': test_url,
                        'success': result['success'],
                        'title': result.get('title', 'N/A'),
                        'word_count': result.get('word_count', 0),
                        'quality_score': result.get('quality_score', 0)
                    })
                else:
                    results.append({
                        'url': test_url,
                        'success': False,
                        'error': 'ìŠ¤í¬ë˜í¼ì— scrape_url ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤'
                    })
            except Exception as e:
                results.append({
                    'url': test_url,
                    'success': False,
                    'error': str(e)
                })
        
        return jsonify({
            'success': True,
            'message': 'í…ŒìŠ¤íŠ¸ ì™„ë£Œ',
            'scraper_type': _scraper_type,
            'test_results': results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@web_scraper_bp.route('/capabilities', methods=['GET'])
def scraper_capabilities():
    """ìŠ¤í¬ë˜í¼ ê¸°ëŠ¥ ì •ë³´"""
    try:
        scraper = get_scraper()
        
        capabilities = {
            'scraper_type': _scraper_type,
            'basic_scraping': hasattr(scraper, 'scrape_url'),
            'professional_scraping': hasattr(scraper, 'scrape_and_store_professionally'),
            'health_check': hasattr(scraper, 'health_check'),
            'cosmos_db_storage': hasattr(scraper, 'cosmos_service'),
            'ai_analysis': hasattr(scraper, 'ai_keywords'),
            'quality_scoring': hasattr(scraper, 'calculate_quality_score'),
            'semantic_chunking': hasattr(scraper, 'create_professional_chunks')
        }
        
        features = []
        if capabilities['basic_scraping']:
            features.append('Basic web scraping')
        if capabilities['professional_scraping']:
            features.append('Professional content extraction')
        if capabilities['cosmos_db_storage']:
            features.append('Cosmos DB storage')
        if capabilities['ai_analysis']:
            features.append('AI relevance analysis')
        if capabilities['quality_scoring']:
            features.append('Content quality scoring')
        if capabilities['semantic_chunking']:
            features.append('Semantic text chunking')
        
        return jsonify({
            'success': True,
            'scraper_type': _scraper_type,
            'capabilities': capabilities,
            'available_features': features,
            'endpoints': {
                'health': '/api/scraper/health',
                'scrape': '/api/scraper/scrape',
                'simple_scrape': '/api/scraper/scrape/simple',
                'test': '/api/scraper/test',
                'capabilities': '/api/scraper/capabilities'
            },
            'usage_examples': {
                'professional_scraping': {
                    'url': '/api/scraper/scrape',
                    'method': 'POST',
                    'body': {
                        'url': 'https://example.com/article',
                        'store_to_db': True,
                        'use_professional': True
                    }
                },
                'simple_scraping': {
                    'url': '/api/scraper/scrape/simple',
                    'method': 'POST',
                    'body': {
                        'url': 'https://example.com/article'
                    }
                }
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@web_scraper_bp.route('/batch', methods=['POST'])
@async_route
async def batch_scrape():
    """ì—¬ëŸ¬ URL ì¼ê´„ ìŠ¤í¬ë˜í•‘"""
    try:
        data = request.get_json()
        if not data or 'urls' not in data:
            return jsonify({
                'success': False,
                'error': 'URLs ë°°ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤',
                'required_format': {'urls': ['https://example1.com', 'https://example2.com']}
            }), 400
        
        urls = data['urls']
        if not isinstance(urls, list) or len(urls) == 0:
            return jsonify({
                'success': False,
                'error': 'URLsëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤'
            }), 400
        
        if len(urls) > 10:  # ì•ˆì „ì„ ìœ„í•œ ì œí•œ
            return jsonify({
                'success': False,
                'error': 'í•œ ë²ˆì— ìµœëŒ€ 10ê°œì˜ URLë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤'
            }), 400
        
        # ì˜µì…˜ íŒŒë¼ë¯¸í„°
        store_to_db = data.get('store_to_db', True)
        use_professional = data.get('use_professional', True)
        
        scraper = get_scraper()
        results = []
        successful = 0
        failed = 0
        
        logger.info(f"ğŸ” ì¼ê´„ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {len(urls)}ê°œ URL")
        
        for i, url in enumerate(urls):
            try:
                url = url.strip()
                if not url.startswith(('http://', 'https://')):
                    results.append({
                        'url': url,
                        'success': False,
                        'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹'
                    })
                    failed += 1
                    continue
                
                # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
                if hasattr(scraper, 'scrape_and_store_professionally') and use_professional and store_to_db:
                    result = await scraper.scrape_and_store_professionally(url)
                elif hasattr(scraper, 'scrape_url'):
                    result = scraper.scrape_url(url)
                else:
                    result = {'success': False, 'error': 'ìŠ¤í¬ë˜í¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
                
                if result['success']:
                    successful += 1
                    # ê²°ê³¼ì—ì„œ í° ë°ì´í„°ëŠ” ìš”ì•½ë§Œ í¬í•¨
                    summary_result = {
                        'url': url,
                        'success': True,
                        'title': result.get('document', {}).get('title') or result.get('title', 'N/A'),
                        'word_count': result.get('document', {}).get('word_count') or result.get('word_count', 0),
                        'quality_score': result.get('document', {}).get('quality_score') or result.get('quality_score', 0)
                    }
                    
                    if 'chunks' in result:
                        summary_result['chunks_created'] = result['chunks']['total_created']
                        summary_result['chunks_stored'] = result['chunks']['stored_successfully']
                    
                    results.append(summary_result)
                else:
                    failed += 1
                    results.append({
                        'url': url,
                        'success': False,
                        'error': result.get('error', 'ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨')
                    })
                
                logger.info(f"  ğŸ“„ {i+1}/{len(urls)} ì™„ë£Œ: {url}")
                
            except Exception as e:
                failed += 1
                results.append({
                    'url': url,
                    'success': False,
                    'error': str(e)
                })
                logger.error(f"  âŒ {i+1}/{len(urls)} ì‹¤íŒ¨: {url} - {e}")
        
        return jsonify({
            'success': True,
            'message': f'ì¼ê´„ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {successful}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨',
            'summary': {
                'total_urls': len(urls),
                'successful': successful,
                'failed': failed,
                'success_rate': round((successful / len(urls)) * 100, 1)
            },
            'results': results,
            'scraper_type': _scraper_type,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ ì¼ê´„ ìŠ¤í¬ë˜í•‘ API ì˜¤ë¥˜: {e}")
        return jsonify({
            'success': False,
            'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@web_scraper_bp.route('/stats', methods=['GET'])
def scraper_stats():
    """ìŠ¤í¬ë˜í¼ í†µê³„ ì •ë³´"""
    try:
        scraper = get_scraper()
        
        stats = {
            'scraper_type': _scraper_type,
            'initialization_time': datetime.now().isoformat(),
            'features': {
                'professional_mode': hasattr(scraper, 'scrape_and_store_professionally'),
                'cosmos_db_integration': hasattr(scraper, 'cosmos_service'),
                'ai_analysis': hasattr(scraper, 'ai_keywords'),
                'quality_scoring': hasattr(scraper, 'calculate_quality_score')
            }
        }
        
        # AI í‚¤ì›Œë“œ í†µê³„ (ìˆëŠ” ê²½ìš°)
        if hasattr(scraper, 'ai_keywords'):
            stats['ai_keywords'] = {
                'total_keywords': len(scraper.ai_keywords),
                'top_keywords': list(scraper.ai_keywords.keys())[:10]
            }
        
        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

# Error handlers specific to web scraper
@web_scraper_bp.errorhandler(400)
def scraper_bad_request(error):
    return jsonify({
        'success': False,
        'error': 'Bad request to web scraper',
        'message': 'ìš”ì²­ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”',
        'required_format': {
            'scrape': {'url': 'https://example.com'},
            'batch': {'urls': ['https://example1.com', 'https://example2.com']}
        },
        'timestamp': datetime.now().isoformat()
    }), 400

@web_scraper_bp.errorhandler(500)
def scraper_internal_error(error):
    logger.error(f"Web scraper internal error: {error}")
    return jsonify({
        'success': False,
        'error': 'Web scraper internal error',
        'message': 'ì›¹ ìŠ¤í¬ë˜í¼ì—ì„œ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
        'timestamp': datetime.now().isoformat()
    }), 500

# ë…ë¦½ ì‹¤í–‰ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == '__main__':
    print("ğŸ§ª Enhanced ì›¹ ìŠ¤í¬ë˜í¼ ë¼ìš°íŠ¸ ë…ë¦½ í…ŒìŠ¤íŠ¸")
    
    try:
        # ê²½ë¡œ ì„¤ì •
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        
        # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        scraper = get_scraper()
        print(f"âœ… ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì„±ê³µ: {_scraper_type}")
        
        # ê°„ë‹¨í•œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        if hasattr(scraper, 'scrape_url'):
            test_result = scraper.scrape_url("https://httpbin.org/html")
            
            if test_result['success']:
                print("âœ… ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
                print(f"ì œëª©: {test_result['title']}")
                print(f"ë‚´ìš© ê¸¸ì´: {len(test_result['content'])} ë¬¸ì")
            else:
                print(f"âŒ ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_result['error']}")
        
        # ì „ë¬¸ì ì¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ë¹„ë™ê¸°)
        if hasattr(scraper, 'scrape_and_store_professionally'):
            print("âœ… ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥")
        else:
            print("âš ï¸ ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ ê¸°ëŠ¥ ì—†ìŒ (ê°„ë‹¨í•œ ëª¨ë“œ)")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()