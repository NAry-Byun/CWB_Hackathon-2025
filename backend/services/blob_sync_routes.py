# routes/blob_sync_routes.py - Flask Backendë¡œ Blob Storage ë™ê¸°í™” (ìˆ˜ì •ëœ ë²„ì „)

from flask import Blueprint, request, jsonify
import asyncio
import logging
from datetime import datetime
from functools import wraps

logger = logging.getLogger(__name__)

# Blueprint ìƒì„±
blob_sync_bp = Blueprint('blob_sync', __name__)

def async_route(f):
    """Flask routeë¥¼ async í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(f(*args, **kwargs))
        finally:
            loop.close()
    return wrapper

@blob_sync_bp.route('/sync-all', methods=['POST'])
@async_route
async def sync_all_blobs():
    """ëª¨ë“  Blob Storage íŒŒì¼ì„ Cosmos DBë¡œ ë™ê¸°í™”"""
    try:
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from services.azure_storage_service import AzureStorageService
        from services.cosmos_service import CosmosVectorService
        from services.azure_openai_service import AzureOpenAIService
        from services.document_processor import DocumentProcessor
        
        storage_service = AzureStorageService()
        cosmos_service = CosmosVectorService()
        openai_service = AzureOpenAIService()
        doc_processor = DocumentProcessor()
        
        await cosmos_service.initialize_database()
        
        # Blob Storageì—ì„œ ëª¨ë“  íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ” Blob Storage íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°...")
        files = await storage_service.list_files()
        
        results = {
            "processed_files": [],
            "failed_files": [],
            "total_files": len(files),
            "total_chunks": 0
        }
        
        for file_info in files:
            try:
                filename = file_info['name']
                logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}")
                
                # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸
                if not doc_processor.validate_file_format(filename):
                    logger.info(f"â­ï¸ ê±´ë„ˆë›°ê¸° (ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹): {filename}")
                    continue
                
                # Cosmos DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                existing = await check_file_exists_in_cosmos(cosmos_service, filename)
                if existing:
                    logger.info(f"â­ï¸ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì¡´ì¬): {filename}")
                    continue
                
                # Blobì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                file_content = await storage_service.download_file(filename)
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = await doc_processor.extract_text_from_file(
                    file_content, filename
                )
                
                if len(text_content.strip()) < 100:
                    logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {filename}")
                    continue
                
                # í…ìŠ¤íŠ¸ ì²­í‚¹
                chunks = split_text_into_chunks(text_content)
                
                # ê° ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ê³  ì €ì¥
                chunk_count = 0
                for i, chunk in enumerate(chunks):
                    # ì„ë² ë”© ìƒì„±
                    embedding = await openai_service.generate_embeddings(chunk)
                    
                    # Cosmos DBì— ì €ì¥
                    await cosmos_service.store_document_chunk(
                        file_name=filename,
                        chunk_text=chunk,
                        embedding=embedding,
                        chunk_index=i,
                        metadata={
                            "file_size": file_info.get('size', 0),
                            "last_modified": file_info.get('last_modified'),
                            "content_type": file_info.get('content_type'),
                            "source": "blob_storage"
                        }
                    )
                    chunk_count += 1
                
                results["processed_files"].append({
                    "filename": filename,
                    "chunks_created": chunk_count,
                    "file_size": file_info.get('size', 0)
                })
                results["total_chunks"] += chunk_count
                
                logger.info(f"âœ… ì™„ë£Œ: {filename} ({chunk_count} chunks)")
                
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {filename}: {str(e)}")
                results["failed_files"].append({
                    "filename": filename,
                    "error": str(e)
                })
        
        return jsonify({
            "success": True,
            "message": f"{len(results['processed_files'])} íŒŒì¼ ë™ê¸°í™” ì™„ë£Œ",
            "results": results,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@blob_sync_bp.route('/sync-file', methods=['POST'])
@async_route
async def sync_single_file():
    """íŠ¹ì • íŒŒì¼ í•˜ë‚˜ë§Œ ë™ê¸°í™”"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        
        if not filename:
            return jsonify({"error": "filenameì´ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from services.azure_storage_service import AzureStorageService
        from services.cosmos_service import CosmosVectorService
        from services.azure_openai_service import AzureOpenAIService
        from services.document_processor import DocumentProcessor
        
        storage_service = AzureStorageService()
        cosmos_service = CosmosVectorService()
        openai_service = AzureOpenAIService()
        doc_processor = DocumentProcessor()
        
        await cosmos_service.initialize_database()
        
        # íŒŒì¼ ì²˜ë¦¬
        logger.info(f"ğŸ“„ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬: {filename}")
        
        # Blobì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        file_content = await storage_service.download_file(filename)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_content = await doc_processor.extract_text_from_file(
            file_content, filename
        )
        
        # ì²­í‚¹ ë° ë²¡í„°í™”
        chunks = split_text_into_chunks(text_content)
        chunk_count = 0
        
        for i, chunk in enumerate(chunks):
            embedding = await openai_service.generate_embeddings(chunk)
            
            await cosmos_service.store_document_chunk(
                file_name=filename,
                chunk_text=chunk,
                embedding=embedding,
                chunk_index=i,
                metadata={"source": "blob_storage_manual"}
            )
            chunk_count += 1
        
        return jsonify({
            "success": True,
            "message": f"íŒŒì¼ '{filename}' ë™ê¸°í™” ì™„ë£Œ",
            "chunks_created": chunk_count,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ ë‹¨ì¼ íŒŒì¼ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@blob_sync_bp.route('/status', methods=['GET'])
@async_route
async def sync_status():
    """ë™ê¸°í™” ìƒíƒœ í™•ì¸"""
    try:
        from services.azure_storage_service import AzureStorageService
        from services.cosmos_service import CosmosVectorService
        
        storage_service = AzureStorageService()
        cosmos_service = CosmosVectorService()
        
        await cosmos_service.initialize_database()
        
        # Blob Storage íŒŒì¼ ìˆ˜
        blob_files = await storage_service.list_files()
        blob_count = len(blob_files)
        
        # Cosmos DB ë¬¸ì„œ ìˆ˜
        cosmos_stats = await cosmos_service.get_document_stats()
        
        # ë™ê¸°í™”ë˜ì§€ ì•Šì€ íŒŒì¼ ì°¾ê¸°
        not_synced = []
        for file_info in blob_files:
            filename = file_info['name']
            exists = await check_file_exists_in_cosmos(cosmos_service, filename)
            if not exists:
                not_synced.append(filename)
        
        return jsonify({
            "success": True,
            "status": {
                "blob_storage_files": blob_count,
                "cosmos_db_documents": cosmos_stats.get('total_documents', 0),
                "not_synced_files": len(not_synced),
                "not_synced_list": not_synced[:10],  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                "sync_percentage": ((blob_count - len(not_synced)) / blob_count * 100) if blob_count > 0 else 0
            },
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@blob_sync_bp.route('/health', methods=['GET'])
def health_check():
    """Blob ë™ê¸°í™” ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "healthy",
        "service": "blob_sync",
        "endpoints": [
            "/sync-all",
            "/sync-file", 
            "/status",
            "/health"
        ],
        "timestamp": datetime.now().isoformat()
    })

# í—¬í¼ í•¨ìˆ˜ë“¤
async def check_file_exists_in_cosmos(cosmos_service, filename: str) -> bool:
    """Cosmos DBì— íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    try:
        # hasattrë¡œ ë©”ì„œë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if hasattr(cosmos_service, 'check_file_exists'):
            return await cosmos_service.check_file_exists(filename)
        else:
            # ëŒ€ì•ˆ: ì§ì ‘ ì¿¼ë¦¬ë¡œ í™•ì¸
            container = cosmos_service.container
            query = f"SELECT VALUE COUNT(1) FROM c WHERE c.file_name = '{filename}'"
            items = list(container.query_items(
                query=query,
                enable_cross_partition_query=True
            ))
            return items[0] > 0 if items else False
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return False

def split_text_into_chunks(text: str, max_chunk_size: int = 1000, overlap: int = 200) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    words = text.split()
    
    current_chunk = []
    current_length = 0
    
    for word in words:
        if current_length + len(word) + 1 <= max_chunk_size:
            current_chunk.append(word)
            current_length += len(word) + 1
        else:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
                
                # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ë‹¨ì–´ ìœ ì§€
                overlap_words = max(0, min(overlap // 10, len(current_chunk) // 2))
                current_chunk = current_chunk[-overlap_words:] if overlap_words > 0 else []
                current_length = sum(len(w) + 1 for w in current_chunk)
                
            current_chunk.append(word)
            current_length += len(word) + 1
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks