"""
í™˜ê²½ë³€ìˆ˜ ë¡œë”© ë¬¸ì œë¥¼ í•´ê²°í•œ ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼
.env íŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import sys
import os
import requests
from bs4 import BeautifulSoup
import re
import time
import hashlib
import json
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
from dataclasses import dataclass, field

# í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ë¡œë”©
def load_env_file():
    """í™˜ê²½ë³€ìˆ˜ íŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ"""
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ .env íŒŒì¼ ì°¾ê¸°
        possible_paths = [
            '.env',
            '../.env',
            '../../.env',
            os.path.join(os.path.dirname(__file__), '.env'),
            os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
        ]
        
        env_file_path = None
        for path in possible_paths:
            if os.path.exists(path):
                env_file_path = path
                break
        
        if not env_file_path:
            print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“ .env íŒŒì¼ ë°œê²¬: {env_file_path}")
        
        # .env íŒŒì¼ ì½ê¸°
        with open(env_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    
                    if key and value:
                        os.environ[key] = value
                        print(f"âœ… í™˜ê²½ë³€ìˆ˜ ì„¤ì •: {key}")
        
        return True
        
    except Exception as e:
        print(f"âŒ .env íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

# .env íŒŒì¼ ë¡œë“œ
print("ğŸ”§ í™˜ê²½ë³€ìˆ˜ ë¡œë”© ì‹œì‘...")
load_env_file()

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
required_vars = ['COSMOS_DB_ENDPOINT', 'COSMOS_DB_KEY']
missing_vars = []

for var in required_vars:
    if not os.environ.get(var):
        missing_vars.append(var)
    else:
        print(f"âœ… {var}: {'*' * 10}...{os.environ[var][-10:]}")

if missing_vars:
    print(f"âŒ ëˆ„ë½ëœ í™˜ê²½ë³€ìˆ˜: {', '.join(missing_vars)}")
    print("ğŸ“ .env íŒŒì¼ì— ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”:")
    for var in missing_vars:
        print(f"   {var}=your_value_here")

# ì´ì œ ì„œë¹„ìŠ¤ë“¤ import (ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •)
try:
    from services.cosmos_service import CosmosVectorService
    print("âœ… services.cosmos_service ì„±ê³µì ìœ¼ë¡œ import")
except ImportError as e:
    print(f"âŒ services.cosmos_service import ì‹¤íŒ¨: {e}")
    print("âš ï¸ ëŒ€ì•ˆ: ê°„ë‹¨í•œ ë”ë¯¸ ì„œë¹„ìŠ¤ ì‚¬ìš©")
    
    class DummyCosmosService:
        async def initialize_database(self):
            print("ğŸ¤– ë”ë¯¸ Cosmos DB ì´ˆê¸°í™”")
            return True
        
        async def store_document_chunk(self, file_name, chunk_text, embedding, chunk_index, metadata):
            print(f"ğŸ¤– ë”ë¯¸ ë¬¸ì„œ ì²­í¬ ì €ì¥: {file_name}[{chunk_index}]")
            return f"dummy_doc_id_{chunk_index}"
    
    CosmosVectorService = DummyCosmosService

# OpenAI ì„œë¹„ìŠ¤
try:
    from services.azure_openai_service import AzureOpenAIService
    openai_service_class = AzureOpenAIService
    print("âœ… services.azure_openai_service ë°œê²¬")
except ImportError:
    try:
        from services.openai_service import OpenAIService
        openai_service_class = OpenAIService
        print("âœ… services.openai_service ë°œê²¬")
    except ImportError:
     print("âš ï¸ OpenAI ì„œë¹„ìŠ¤ë“¤ ì—†ìŒ. ë”ë¯¸ ì„œë¹„ìŠ¤ ì‚¬ìš©")
    
    class DummyOpenAIService:
        async def generate_embeddings(self, text):
            print(f"ğŸ¤– ë”ë¯¸ ì„ë² ë”© ìƒì„±: {len(text)} ë¬¸ì")
            import random
            return [random.random() for _ in range(1536)]
        
        async def generate_response(self, *args, **kwargs):
            return {
                "assistant_message": "ë”ë¯¸ ì‘ë‹µì…ë‹ˆë‹¤.",
                "content": "ë”ë¯¸ ì‘ë‹µì…ë‹ˆë‹¤."
            }
    
    openai_service_class = DummyOpenAIService

@dataclass
class ProfessionalDocument:
    """ì „ë¬¸ì ì¸ ë¬¸ì„œ êµ¬ì¡°"""
    url: str
    title: str
    content: str
    author: Optional[str] = None
    content_type: str = "blog_post"
    word_count: int = 0
    quality_score: float = 0.0
    ai_relevance_score: float = 0.0
    key_concepts: List[str] = field(default_factory=list)
    summary: str = ""

class EnvironmentFixedScraper:
    """í™˜ê²½ë³€ìˆ˜ ë¬¸ì œê°€ í•´ê²°ëœ ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, cosmos_service=None, openai_service=None):
        # ì„œë¹„ìŠ¤ê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„œë¹„ìŠ¤ ìƒì„±
        if cosmos_service is None:
            try:
                self.cosmos_service = CosmosVectorService()
            except Exception as e:
                print(f"âš ï¸ Cosmos ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.cosmos_service = DummyCosmosService()
        else:
            self.cosmos_service = cosmos_service
            
        if openai_service is None:
            try:
                self.openai_service = openai_service_class()
            except Exception as e:
                print(f"âš ï¸ OpenAI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.openai_service = DummyOpenAIService()
        else:
            self.openai_service = openai_service
        
        # HTTP ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive'
        })
        
        # AI í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨)
        self.ai_keywords = {
            'artificial intelligence': 5.0,
            'machine learning': 4.5,
            'deep learning': 4.0,
            'neural network': 3.5,
            'natural language processing': 3.5,
            'computer vision': 3.0,
            'ai': 3.0,
            'ml': 2.5,
            'nlp': 2.5,
            'algorithm': 2.0,
            'automation': 1.5,
            'cognitive': 2.0,
            'intelligent': 1.5
        }
        
        if hasattr(logging, 'getLogger'):
            logger = logging.getLogger(__name__)
            logger.info("ğŸš€ í™˜ê²½ë³€ìˆ˜ ë¬¸ì œê°€ í•´ê²°ëœ ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ")

    async def scrape_and_store_professionally(self, url: str) -> Dict[str, Any]:
        """ì „ë¬¸ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘í•˜ê³  ì €ì¥"""
        try:
            print(f"ğŸ” ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            # 1. ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 2. ì „ë¬¸ì ì¸ ë¬¸ì„œ ì¶”ì¶œ
            document = self.extract_professional_document(soup, url)
            
            # 3. í’ˆì§ˆ ê²€ì¦
            if document.quality_score < 30:
                return {
                    'success': False,
                    'url': url,
                    'error': f'í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ìŒ: {document.quality_score}/100',
                    'quality_score': document.quality_score
                }
            
            # 4. ì „ë¬¸ì ì¸ ì²­í¬ ìƒì„±
            chunks = self.create_professional_chunks(document)
            
            if not chunks:
                return {
                    'success': False,
                    'url': url,
                    'error': 'ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŒ'
                }
            
            # 5. Cosmos DBì— ì €ì¥
            storage_result = await self.store_with_rich_metadata(document, chunks)
            
            # 6. ê²°ê³¼ ë°˜í™˜
            result = {
                'success': True,
                'url': url,
                'document': {
                    'title': document.title,
                    'author': document.author,
                    'word_count': document.word_count,
                    'quality_score': round(document.quality_score, 1),
                    'ai_relevance_score': round(document.ai_relevance_score, 1),
                    'key_concepts': document.key_concepts,
                    'summary': document.summary[:200] + "..." if len(document.summary) > 200 else document.summary
                },
                'chunks': {
                    'total_created': len(chunks),
                    'stored_successfully': storage_result.get('successful_chunks', 0),
                    'success_rate': storage_result.get('success_rate', 0),
                    'chunk_types': list(set(chunk['chunk_type'] for chunk in chunks))
                },
                'storage': storage_result
            }
            
            print(f"âœ… ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {document.title}")
            print(f"ğŸ“Š í’ˆì§ˆ: {document.quality_score:.1f}/100, AI ê´€ë ¨ì„±: {document.ai_relevance_score:.1f}/100")
            print(f"ğŸ“ ì €ì¥: {storage_result.get('successful_chunks', 0)}/{len(chunks)} ì²­í¬")
            
            return result
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'url': url,
                'error': f'ì›¹ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}',
                'error_type': 'network_error'
            }
        except Exception as e:
            print(f"âŒ ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'url': url,
                'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
                'error_type': 'processing_error'
            }

    def extract_professional_document(self, soup: BeautifulSoup, url: str) -> ProfessionalDocument:
        """ì „ë¬¸ì ì¸ ë¬¸ì„œ ì¶”ì¶œ"""
        
        # ë…¸ì´ì¦ˆ ì œê±°
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        title = self.extract_title(soup)
        content = self.extract_clean_content(soup)
        author = self.extract_author(soup)
        
        # ë¶„ì„
        word_count = len(content.split())
        key_concepts = self.extract_key_concepts(content, title)
        summary = self.generate_summary(content, title)
        quality_score = self.calculate_quality_score(content, title, word_count)
        ai_relevance_score = self.calculate_ai_relevance(content, title)
        
        return ProfessionalDocument(
            url=url,
            title=title,
            content=content,
            author=author,
            word_count=word_count,
            quality_score=quality_score,
            ai_relevance_score=ai_relevance_score,
            key_concepts=key_concepts,
            summary=summary
        )

    def extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        # ìš°ì„ ìˆœìœ„ë³„ ì œëª© ì¶”ì¶œ
        strategies = [
            lambda: soup.select_one('article h1, .post-title h1, .entry-title h1'),
            lambda: soup.find('h1'),
            lambda: soup.find('meta', property='og:title'),
            lambda: soup.find('meta', attrs={'name': 'twitter:title'}),
            lambda: soup.title
        ]
        
        for strategy in strategies:
            try:
                element = strategy()
                if element:
                    if element.name == 'meta':
                        title_text = element.get('content', '').strip()
                    else:
                        title_text = element.get_text(strip=True)
                    
                    if title_text and len(title_text) > 5:
                        # ì‚¬ì´íŠ¸ëª… ì œê±°
                        clean_title = re.sub(r'\s*[\|\-â€“â€”]\s*[^|\-â€“â€”]*$', '', title_text)
                        return clean_title if clean_title else title_text
            except:
                continue
        
        return "Professional Article"

    def extract_clean_content(self, soup: BeautifulSoup) -> str:
        """ê¹¨ë—í•œ ì½˜í…ì¸  ì¶”ì¶œ"""
        content_selectors = [
            'article .post-content',
            'article .entry-content',
            'article .content',
            'main article',
            '.blog-post-content',
            '.post-body',
            'article',
            'main',
            '[role="main"]',
            '.main-content',
            '#main-content'
        ]
        
        best_content = ""
        best_score = 0
        
        for selector in content_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    content_text = element.get_text(separator='\n\n', strip=True)
                    score = self.score_content_quality(content_text)
                    
                    if score > best_score and len(content_text) > 300:
                        best_score = score
                        best_content = content_text
            except:
                continue
        
        # í´ë°±
        if not best_content or len(best_content) < 300:
            if soup.body:
                best_content = soup.body.get_text(separator='\n\n', strip=True)
            else:
                best_content = soup.get_text(separator='\n\n', strip=True)
        
        return self.clean_text(best_content)

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ê³µë°± ì •ê·œí™”
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        
        # ì›¹ ì•„í‹°íŒ©íŠ¸ ì œê±° (AI ì‘ë‹µ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´)
        patterns = [
            r'(Click here|Read more|Continue reading|Subscribe|Share)',
            r'(Facebook|Twitter|LinkedIn|Instagram)\s*(share|button)?',
            r'(Cookie|Privacy)\s*(Policy|Notice)',
            r'https?://\S+',
            r'www\.\S+',
            r'Sources?:\s*web_\S+',
            r'web_www\.\S+',
            r'\([0-9]+%\)',  # í¼ì„¼íŠ¸ ì œê±° (í•µì‹¬!)
            r'Sources?:\s*[^\n]*%[^\n]*'  # ì†ŒìŠ¤ ë¼ì¸ ì œê±° (í•µì‹¬!)
        ]
        
        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ì •ë¦¬
        text = re.sub(r'[.]{3,}', '...', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()

    def extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ì €ì ì¶”ì¶œ"""
        selectors = ['.author-name', '.post-author', '.byline', '.author']
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                author = element.get_text(strip=True)
                if author and 3 <= len(author) <= 50:
                    return author
        
        author_meta = soup.find('meta', attrs={'name': 'author'})
        if author_meta and author_meta.get('content'):
            return author_meta.get('content').strip()
        
        return None

    def extract_key_concepts(self, content: str, title: str) -> List[str]:
        """í‚¤ ì»¨ì…‰ ì¶”ì¶œ"""
        combined_text = f"{title} {content}".lower()
        found_concepts = []
        
        for concept, weight in self.ai_keywords.items():
            if concept in combined_text:
                found_concepts.append(concept)
        
        # ì¤‘ìš”ë„ìˆœ ì •ë ¬
        concept_scores = [(concept, self.ai_keywords[concept]) for concept in found_concepts]
        concept_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [concept for concept, _ in concept_scores[:10]]

    def generate_summary(self, content: str, title: str) -> str:
        """ìš”ì•½ ìƒì„±"""
        sentences = re.split(r'[.!?]+', content)
        meaningful_sentences = [s.strip() for s in sentences if 50 <= len(s.strip()) <= 300]
        
        if meaningful_sentences:
            summary = '. '.join(meaningful_sentences[:3])
            if summary and not summary.endswith('.'):
                summary += '.'
            return summary[:400] + ("..." if len(summary) > 400 else "")
        
        return f"This article discusses {title.lower()} and related concepts."

    def score_content_quality(self, content: str) -> float:
        """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜"""
        if not content:
            return 0.0
        
        score = 0.0
        word_count = len(content.split())
        
        # ê¸¸ì´ ì ìˆ˜
        if 500 <= word_count <= 3000:
            score += 10.0
        elif 200 <= word_count < 500:
            score += 8.0
        
        # êµ¬ì¡° ì ìˆ˜
        paragraph_count = len([p for p in content.split('\n\n') if p.strip()])
        if paragraph_count >= 5:
            score += 5.0
        
        # AI ê´€ë ¨ì„±
        ai_terms = sum(1 for keyword in self.ai_keywords if keyword in content.lower())
        score += min(ai_terms * 0.5, 5.0)
        
        return score

    def calculate_quality_score(self, content: str, title: str, word_count: int) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ë‹¨ì–´ ìˆ˜ (40ì )
        if 800 <= word_count <= 3000:
            score += 40.0
        elif 400 <= word_count < 800:
            score += 35.0
        elif 200 <= word_count < 400:
            score += 25.0
        elif word_count >= 100:
            score += 15.0
        
        # êµ¬ì¡° (30ì )
        paragraphs = [p for p in content.split('\n\n') if p.strip()]
        if len(paragraphs) >= 7:
            score += 30.0
        elif len(paragraphs) >= 5:
            score += 25.0
        elif len(paragraphs) >= 3:
            score += 20.0
        
        # ì œëª© í’ˆì§ˆ (15ì )
        if title and len(title) > 15:
            score += 15.0
        elif title and len(title) > 8:
            score += 10.0
        
        # AI ê´€ë ¨ì„± (15ì )
        ai_terms = sum(1 for keyword in self.ai_keywords if keyword in content.lower())
        score += min(ai_terms * 2, 15.0)
        
        return min(score, 100.0)

    def calculate_ai_relevance(self, content: str, title: str) -> float:
        """AI ê´€ë ¨ì„± ê³„ì‚°"""
        combined_text = f"{title} {title} {content}".lower()  # ì œëª© ê°€ì¤‘ì¹˜
        
        relevance_score = 0.0
        total_weight = 0.0
        
        for keyword, weight in self.ai_keywords.items():
            count = combined_text.count(keyword)
            total_weight += count * weight
        
        word_count = len(combined_text.split())
        if word_count > 0:
            density = (total_weight / word_count) * 1000  # per 1000 words
            relevance_score = min(density * 15, 100.0)
        
        return relevance_score

    def create_professional_chunks(self, document: ProfessionalDocument) -> List[Dict[str, Any]]:
        """ì „ë¬¸ì ì¸ ì²­í¬ ìƒì„±"""
        chunks = []
        
        # 1. ì†Œê°œ ì²­í¬ (ì œëª© + ìš”ì•½ + ì£¼ìš” ê°œë…)
        intro_parts = [
            f"Document: {document.title}",
            f"Summary: {document.summary}",
            f"Key AI concepts: {', '.join(document.key_concepts[:5])}"
        ]
        
        paragraphs = [p.strip() for p in document.content.split('\n\n') if p.strip()]
        if paragraphs:
            intro_parts.append(f"Content: {paragraphs[0]}")
        
        intro_text = '\n\n'.join(intro_parts)
        
        chunks.append({
            'text': intro_text,
            'chunk_type': 'comprehensive_introduction',
            'importance_level': 'high',
            'concepts': document.key_concepts[:3],
            'metadata': {
                'chunk_purpose': 'comprehensive_introduction',
                'contains_title': True,
                'contains_summary': True,
                'contains_key_concepts': True
            }
        })
        
        # 2. ì½˜í…ì¸  ì²­í¬ë“¤ (ì˜ë¯¸ì  ë¶„í• )
        remaining_content = '\n\n'.join(paragraphs[1:]) if len(paragraphs) > 1 else ""
        
        if remaining_content and len(remaining_content) > 300:
            chunk_size = 1200
            sentences = re.split(r'[.!?]+\s+', remaining_content)
            
            current_chunk = ""
            chunk_index = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                test_chunk = current_chunk + " " + sentence if current_chunk else sentence
                
                if len(test_chunk) <= chunk_size:
                    current_chunk = test_chunk
                else:
                    if current_chunk and len(current_chunk) > 300:
                        chunk_text = f"Section {chunk_index + 1} of {document.title}:\n\n{current_chunk.strip()}"
                        
                        chunks.append({
                            'text': chunk_text,
                            'chunk_type': 'content_section',
                            'importance_level': 'medium',
                            'concepts': self.extract_chunk_concepts(current_chunk, document.key_concepts),
                            'metadata': {
                                'chunk_purpose': 'semantic_content_section',
                                'section_number': chunk_index + 1,
                                'document_title': document.title
                            }
                        })
                        chunk_index += 1
                    
                    # ì˜¤ë²„ë©ì„ ìœ„í•œ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ìœ ì§€
                    last_sentences = current_chunk.split('.')[-2:] if current_chunk else []
                    overlap_text = '. '.join(s.strip() for s in last_sentences if s.strip())
                    current_chunk = overlap_text + ". " + sentence if overlap_text else sentence
            
            # ë§ˆì§€ë§‰ ì²­í¬
            if current_chunk and len(current_chunk) > 300:
                chunk_text = f"Final section of {document.title}:\n\n{current_chunk.strip()}"
                
                chunks.append({
                    'text': chunk_text,
                    'chunk_type': 'content_section',
                    'importance_level': 'medium',
                    'concepts': self.extract_chunk_concepts(current_chunk, document.key_concepts),
                    'metadata': {
                        'chunk_purpose': 'semantic_content_section',
                        'section_number': chunk_index + 1,
                        'is_final_section': True
                    }
                })
        
        # 3. ì¢…í•© ìš”ì•½ ì²­í¬ (ê¸´ ë¬¸ì„œì˜ ê²½ìš°)
        if document.word_count > 1000:
            summary_parts = [
                f"Complete Summary of {document.title}",
                f"Overview: {document.summary}",
                f"Main AI concepts covered: {', '.join(document.key_concepts[:8])}",
                f"Document statistics: {document.word_count} words"
            ]
            
            summary_text = '\n\n'.join(summary_parts)
            
            chunks.append({
                'text': summary_text,
                'chunk_type': 'comprehensive_summary',
                'importance_level': 'high',
                'concepts': document.key_concepts,
                'metadata': {
                    'chunk_purpose': 'comprehensive_summary',
                    'document_overview': True
                }
            })
        
        return chunks

    def extract_chunk_concepts(self, chunk_text: str, document_concepts: List[str]) -> List[str]:
        """ì²­í¬ë³„ ê°œë… ì¶”ì¶œ"""
        chunk_lower = chunk_text.lower()
        present_concepts = []
        
        for concept in document_concepts:
            if concept.lower() in chunk_lower:
                present_concepts.append(concept)
        
        return present_concepts

    async def store_with_rich_metadata(self, document: ProfessionalDocument, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í’ë¶€í•œ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥"""
        try:
            await self.cosmos_service.initialize_database()
            
            successful_chunks = []
            failed_chunks = []
            filename = self.generate_filename(document)
            
            for i, chunk in enumerate(chunks):
                try:
                    # ì„ë² ë”© ìƒì„±
                    embedding = await self.openai_service.generate_embeddings(chunk['text'])
                    
                    if not embedding:
                        failed_chunks.append({'index': i, 'error': 'Embedding generation failed'})
                        continue
                    
                    # ì „ë¬¸ì ì¸ ë©”íƒ€ë°ì´í„° ìƒì„±
                    professional_metadata = {
                        # ë¬¸ì„œ ì •ë³´
                        'source_url': document.url,
                        'document_title': document.title,
                        'document_type': 'environment_fixed_professional_blog',
                        'content_type': document.content_type,
                        'language': 'english',
                        
                        # í’ˆì§ˆ ì •ë³´
                        'quality_level': self.get_quality_level(document.quality_score),
                        'document_quality_score': round(document.quality_score, 1),
                        'ai_relevance_level': self.get_ai_relevance_level(document.ai_relevance_score),
                        'document_ai_relevance_score': round(document.ai_relevance_score, 1),
                        
                        # ì²­í¬ ì •ë³´
                        'chunk_index': i,
                        'total_chunks': len(chunks),
                        'chunk_type': chunk['chunk_type'],
                        'importance_level': chunk['importance_level'],
                        'semantic_context': f"Chunk {i+1} of {len(chunks)} from {document.title}",
                        'chunk_word_count': len(chunk['text'].split()),
                        'chunk_concepts': chunk['concepts'],
                        
                        # ë¬¸ì„œ ë©”íŠ¸ë¦­
                        'document_word_count': document.word_count,
                        'document_summary': document.summary[:100] if document.summary else None,
                        'key_concepts': document.key_concepts,
                        'author': document.author,
                        
                        # ì²˜ë¦¬ ì •ë³´
                        'extraction_method': 'environment_fixed_professional_scraper',
                        'processing_timestamp': datetime.now(timezone.utc).isoformat(),
                        'optimized_for_professional_responses': True,
                        'noise_patterns_removed': True,
                        'semantically_chunked': True,
                        'context_preservation_enabled': True,
                        
                        # ê²€ìƒ‰ ìµœì í™”
                        'searchable_title': document.title.lower(),
                        'searchable_concepts': [concept.lower() for concept in document.key_concepts],
                        'domain': urlparse(document.url).netloc,
                        
                        # AI ì‘ë‹µ íŒíŠ¸
                        'ai_response_hints': {
                            'is_primary_content': chunk['importance_level'] == 'high',
                            'contains_introduction': 'introduction' in chunk['chunk_type'],
                            'contains_summary': 'summary' in chunk['chunk_type'],
                            'has_title_context': chunk['metadata'].get('contains_title', False)
                        },
                        
                        # ì²­í¬ë³„ ë©”íƒ€ë°ì´í„°
                        **chunk['metadata']
                    }
                    
                    # Cosmos DBì— ì €ì¥
                    doc_id = await self.cosmos_service.store_document_chunk(
                        file_name=filename,
                        chunk_text=chunk['text'],
                        embedding=embedding,
                        chunk_index=i,
                        metadata=professional_metadata
                    )
                    
                    successful_chunks.append(doc_id)
                    print(f"âœ… ì „ë¬¸ì ì¸ ë©”íƒ€ë°ì´í„°ë¡œ ì²­í¬ {i+1}/{len(chunks)} ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"âŒ ì²­í¬ {i+1} ì €ì¥ ì‹¤íŒ¨: {e}")
                    failed_chunks.append({'index': i, 'error': str(e)})
            
            success_rate = (len(successful_chunks) / len(chunks) * 100) if chunks else 0
            
            return {
                'successful_chunks': len(successful_chunks),
                'failed_chunks': len(failed_chunks),
                'total_chunks': len(chunks),
                'success_rate': round(success_rate, 1),
                'document_ids': successful_chunks,
                'failed_chunk_details': failed_chunks,
                'filename': filename,
                'metadata_quality': 'professional_grade'
            }
            
        except Exception as e:
            print(f"âŒ ì „ë¬¸ì ì¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return {
                'successful_chunks': 0,
                'failed_chunks': len(chunks),
                'success_rate': 0.0,
                'error': str(e)
            }

    def generate_filename(self, document: ProfessionalDocument) -> str:
        """ì „ë¬¸ì ì¸ íŒŒì¼ëª… ìƒì„±"""
        try:
            domain = urlparse(document.url).netloc.replace('www.', '')
            title_clean = re.sub(r'[^\w\s-]', '', document.title)
            title_clean = re.sub(r'\s+', '_', title_clean).lower()
            
            quality_suffix = self.get_quality_level(document.quality_score)
            
            if title_clean and len(title_clean) > 8:
                filename = f"professional_{domain}_{title_clean}_{quality_suffix}"
            else:
                path_clean = re.sub(r'[^\w-]', '_', urlparse(document.url).path)
                filename = f"professional_{domain}_{path_clean}_{quality_suffix}"
            
            # ê¸¸ì´ ì œí•œ
            filename = filename[:120]
            filename = re.sub(r'_{2,}', '_', filename)
            filename = filename.strip('_')
            
            return filename
            
        except Exception:
            url_hash = hashlib.md5(document.url.encode()).hexdigest()[:12]
            return f"professional_content_{url_hash}"

    def get_quality_level(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë ˆë²¨ë¡œ ë³€í™˜"""
        if score >= 80:
            return 'excellent'
        elif score >= 65:
            return 'very_good'
        elif score >= 50:
            return 'good'
        elif score >= 35:
            return 'acceptable'
        else:
            return 'needs_improvement'

    def get_ai_relevance_level(self, score: float) -> str:
        """AI ê´€ë ¨ì„±ì„ ë ˆë²¨ë¡œ ë³€í™˜"""
        if score >= 70:
            return 'highly_relevant'
        elif score >= 50:
            return 'relevant'
        elif score >= 30:
            return 'moderately_relevant'
        elif score >= 15:
            return 'somewhat_relevant'
        else:
            return 'low_relevance'

    async def health_check(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            health_info = {
                'service_name': 'EnvironmentFixedScraper',
                'status': 'healthy',
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'environment_variables': {},
                'services': {},
                'scraper_config': {}
            }
            
            # í™˜ê²½ë³€ìˆ˜ ìƒíƒœ í™•ì¸
            env_vars = ['COSMOS_DB_ENDPOINT', 'COSMOS_DB_KEY']
            for var in env_vars:
                value = os.environ.get(var)
                if value:
                    health_info['environment_variables'][var] = f"{'*' * 5}...{value[-5:]}"
                else:
                    health_info['environment_variables'][var] = 'NOT_SET'
            
            # ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
            try:
                test_response = self.session.get('https://httpbin.org/status/200', timeout=10)
                health_info['services']['network'] = {
                    'status': 'healthy' if test_response.status_code == 200 else 'unhealthy',
                    'response_time_ms': round(test_response.elapsed.total_seconds() * 1000, 2)
                }
            except Exception as e:
                health_info['services']['network'] = {
                    'status': 'unhealthy',
                    'error': str(e)
                }
                health_info['status'] = 'degraded'
            
            # ì„œë¹„ìŠ¤ ìƒíƒœ
            health_info['services']['cosmos_service'] = {
                'status': 'connected' if self.cosmos_service else 'not_connected',
                'service_type': type(self.cosmos_service).__name__ if self.cosmos_service else 'None'
            }
            
            health_info['services']['openai_service'] = {
                'status': 'connected' if self.openai_service else 'not_connected',
                'service_type': type(self.openai_service).__name__ if self.openai_service else 'None'
            }
            
            # ìŠ¤í¬ë˜í¼ ì„¤ì •
            health_info['scraper_config'] = {
                'ai_keywords_loaded': len(self.ai_keywords),
                'user_agent': self.session.headers.get('User-Agent')[:50] + "..."
            }
            
            return health_info
            
        except Exception as e:
            return {
                'service_name': 'EnvironmentFixedScraper',
                'status': 'unhealthy',
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'error': str(e)
            }

# Simple scraper fallback for basic functionality
class SimpleWebScraper:
    """ê°„ë‹¨í•œ ì›¹ ìŠ¤í¬ë˜í¼ (fallbackìš©)"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.ai_keywords = [
            'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'ai', 'ml', 'algorithm'
        ]

    def scrape_url(self, url: str) -> dict:
        """URLì„ ìŠ¤í¬ë˜í•‘í•˜ê³  ê²°ê³¼ ë°˜í™˜"""
        try:
            print(f"ğŸ” ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            # HTTP ìš”ì²­
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = self.extract_title(soup)
            content = self.extract_content(soup)
            
            # ë¶„ì„
            word_count = len(content.split())
            ai_related = self.is_ai_related(content, title)
            quality_score = self.calculate_quality(content, title, word_count)
            
            result = {
                'success': True,
                'url': url,
                'title': title,
                'content': content[:1000] + "..." if len(content) > 1000 else content,
                'word_count': word_count,
                'ai_related': ai_related,
                'quality_score': quality_score,
                'scraped_at': datetime.now().isoformat()
            }
            
            print(f"âœ… ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {title}")
            return result
            
        except Exception as e:
            print(f"âŒ ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e)
            }

    def extract_title(self, soup):
        """ì œëª© ì¶”ì¶œ"""
        # H1 íƒœê·¸ ì‹œë„
        h1 = soup.find('h1')
        if h1 and h1.get_text(strip=True):
            return h1.get_text(strip=True)
        
        # Title íƒœê·¸ ì‹œë„
        if soup.title:
            return soup.title.get_text(strip=True)
        
        return "Untitled"

    def extract_content(self, soup):
        """ì½˜í…ì¸  ì¶”ì¶œ"""
        # ë…¸ì´ì¦ˆ ì œê±°
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        article = soup.find('article')
        if article:
            content = article.get_text(separator=' ', strip=True)
        elif soup.body:
            content = soup.body.get_text(separator=' ', strip=True)
        else:
            content = soup.get_text(separator=' ', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        content = re.sub(r'\s+', ' ', content)
        return content.strip()

    def is_ai_related(self, content, title):
        """AI ê´€ë ¨ ì—¬ë¶€ í™•ì¸"""
        text = f"{title} {content}".lower()
        return any(keyword in text for keyword in self.ai_keywords)

    def calculate_quality(self, content, title, word_count):
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        
        if word_count >= 200:
            score += 30
        if word_count >= 500:
            score += 20
        if title and len(title) > 10:
            score += 25
        if self.is_ai_related(content, title):
            score += 25
        
        return min(score, 100)


# Factory function to get the appropriate scraper
def get_scraper(use_professional=True):
    """ì ì ˆí•œ ìŠ¤í¬ë˜í¼ë¥¼ ë°˜í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜"""
    if use_professional:
        try:
            return EnvironmentFixedScraper()
        except Exception as e:
            print(f"âš ï¸ ì „ë¬¸ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì‹¤íŒ¨, ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼ ì‚¬ìš©: {e}")
            return SimpleWebScraper()
    else:
        return SimpleWebScraper()


# Test functions
async def run_environment_fixed_scraping():
    """í™˜ê²½ë³€ìˆ˜ ë¬¸ì œê°€ í•´ê²°ëœ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    print("ğŸš€ í™˜ê²½ë³€ìˆ˜ ë¬¸ì œ í•´ê²°ëœ ì „ë¬¸ì ì¸ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘")
    print("=" * 70)
    
    try:
        # ìŠ¤í¬ë˜í¼ ìƒì„±
        scraper = get_scraper(use_professional=True)
        
        if isinstance(scraper, EnvironmentFixedScraper):
            print("âœ… ì „ë¬¸ì ì¸ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì„±ê³µ")
            
            # ìƒíƒœ í™•ì¸
            print("\nğŸ¥ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸...")
            health = await scraper.health_check()
            print(f"ì„œë¹„ìŠ¤ ìƒíƒœ: {health['status']}")
            
            # í™˜ê²½ë³€ìˆ˜ í™•ì¸ ì¶œë ¥
            env_status = health.get('environment_variables', {})
            for var, status in env_status.items():
                if status == 'NOT_SET':
                    print(f"   âŒ {var}: {status}")
                else:
                    print(f"   âœ… {var}: {status}")
            
            # ë„¤íŠ¸ì›Œí¬ ìƒíƒœ
            network_status = health.get('services', {}).get('network', {})
            print(f"   ğŸŒ ë„¤íŠ¸ì›Œí¬: {network_status.get('status', 'unknown')}")
            
            if health['status'] == 'unhealthy':
                print("âŒ ì„œë¹„ìŠ¤ ìƒíƒœê°€ ë¶ˆëŸ‰í•©ë‹ˆë‹¤. ê°„ë‹¨í•œ ìŠ¤í¬ë˜í¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                scraper = SimpleWebScraper()
        
        # URL í…ŒìŠ¤íŠ¸
        test_url = "https://www.artificial-intelligence.blog/terminology/artificial-intelligence"
        
        print(f"\nğŸ“„ ì²˜ë¦¬í•  URL: {test_url}")
        print("ğŸ”„ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ì¤‘...")
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        if isinstance(scraper, EnvironmentFixedScraper):
            result = await scraper.scrape_and_store_professionally(test_url)
        else:
            result = scraper.scrape_url(test_url)
        
        if result['success']:
            print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ ì™„ë£Œ!")
            print("=" * 50)
            
            if isinstance(scraper, EnvironmentFixedScraper):
                # ì „ë¬¸ì ì¸ ê²°ê³¼ ì¶œë ¥
                doc = result['document']
                print(f"ğŸ“‹ ë¬¸ì„œ ì •ë³´:")
                print(f"   ì œëª©: {doc['title']}")
                print(f"   ì €ì: {doc.get('author', 'Unknown')}")
                print(f"   ë‹¨ì–´ ìˆ˜: {doc['word_count']}")
                print(f"   í’ˆì§ˆ ì ìˆ˜: {doc['quality_score']}/100")
                print(f"   AI ê´€ë ¨ì„±: {doc['ai_relevance_score']}/100")
                
                if doc['key_concepts']:
                    print(f"   ì£¼ìš” AI ê°œë…:")
                    for i, concept in enumerate(doc['key_concepts'][:5], 1):
                        print(f"      {i}. {concept}")
                
                if doc['summary']:
                    print(f"   ìš”ì•½: {doc['summary']}")
                
                # ì²­í‚¹ ë¶„ì„
                chunks = result['chunks']
                print(f"\nğŸ“¦ ì²­í‚¹ ë¶„ì„:")
                print(f"   ì´ ìƒì„±ëœ ì²­í¬: {chunks['total_created']}")
                print(f"   ì„±ê³µì ìœ¼ë¡œ ì €ì¥: {chunks['stored_successfully']}")
                print(f"   ì €ì¥ ì„±ê³µë¥ : {chunks['success_rate']}%")
                print(f"   ì²­í¬ íƒ€ì…ë“¤: {', '.join(chunks['chunk_types'])}")
            else:
                # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
                print(f"ğŸ“‹ ë¬¸ì„œ ì •ë³´:")
                print(f"   ì œëª©: {result['title']}")
                print(f"   ë‹¨ì–´ ìˆ˜: {result['word_count']}")
                print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality_score']}/100")
                print(f"   AI ê´€ë ¨: {result['ai_related']}")
                print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {result['content'][:200]}...")
            
        else:
            print(f"\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨:")
            print(f"   ì˜¤ë¥˜: {result['error']}")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {result.get('error_type', 'unknown')}")
    
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ”§ í™˜ê²½ë³€ìˆ˜ ë¬¸ì œ í•´ê²°ëœ ì „ë¬¸ì ì¸ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í¼")
    print("Cosmos DB ì—°ê²° ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìµœê³  í’ˆì§ˆì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    print("=" * 80)
    
    try:
        # ì „ì²´ ì‹¤í–‰
        asyncio.run(run_environment_fixed_scraping())
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")